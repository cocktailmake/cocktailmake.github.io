<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>RICHKA Developer Blog (Posts about Recording)</title><link>https://cocktailmake.github.io/</link><description></description><atom:link href="https://cocktailmake.github.io/categories/cat_recording.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2021 &lt;a href="mailto:"&gt;RICHKA&lt;/a&gt; </copyright><lastBuildDate>Thu, 08 Apr 2021 01:15:56 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Markers of Video Player on Recording Service</title><link>https://cocktailmake.github.io/posts/recording-player-markers/</link><dc:creator>Duc To</dc:creator><description>&lt;div id="outline-container-org39a9fe3" class="outline-2"&gt;
&lt;h2 id="org39a9fe3"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org39a9fe3"&gt;
&lt;p&gt;
In Recording project, we use videojs &lt;a href="https://videojs.com/"&gt;https://videojs.com/&lt;/a&gt; and its 3rd party plugin markers &lt;a href="https://github.com/spchuang/videojs-markers"&gt;https://github.com/spchuang/videojs-markers&lt;/a&gt; to play the video as well as marking the positions of narration texts in which we will play custom audios which are automatically generated with speech synthesis technologies or manually recorded by users.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/recording-player-markers/recording_edit.png" alt="nil"&gt;
&lt;/p&gt;

&lt;p&gt;
The goal of this post is to focus on videojs's markers plugin, how to use it and its common use cases.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org460a0e9" class="outline-2"&gt;
&lt;h2 id="org460a0e9"&gt;Initialization&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org460a0e9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;markers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;9.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;text&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"this"&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="nx"&gt;text&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"is"&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;23.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;text&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"so"&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="nx"&gt;text&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"cool"&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;];&lt;/span&gt;

&lt;span class="nx"&gt;video&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;markers&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
  &lt;span class="nx"&gt;markers&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;markers&lt;/span&gt;
&lt;span class="p"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
In above example, video is an instance of videojs object, 2 compulsory fields are time and text but you can add any additional info which you want to query later. In Recording project, we store one additional data named &lt;code&gt;is_recorded&lt;/code&gt; to mark whether the marker already be recorded audio file for it.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0fd336e" class="outline-2"&gt;
&lt;h2 id="org0fd336e"&gt;Common Cases&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0fd336e"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7f70b16" class="outline-3"&gt;
&lt;h3 id="org7f70b16"&gt;Customize Marker Style&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7f70b16"&gt;
&lt;p&gt;
We can style differently between normal marker and selected marker. Each marker has style class &lt;code&gt;vjs-marker&lt;/code&gt; and the current marker has additional style class &lt;code&gt;vjs-marker-current&lt;/code&gt;. Based on them we can style anything we want or even add additional html code to each marker as the above screenshot.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;div data-marker-key="63e42a41-620d-44b3-b0ee-ff5617ac051d" data-marker-time="1.368872" class="vjs-marker ui-draggable ui-draggable-handle vjs-marker-current"&amp;gt;
  &amp;lt;div class="cue-line"&amp;gt;
    2
  &amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc2f1764" class="outline-3"&gt;
&lt;h3 id="orgc2f1764"&gt;Draggable &amp;amp; Update Marker's Position&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc2f1764"&gt;
&lt;p&gt;
We can support markers to be draggable by using draggable JS library
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;$&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;".vjs-marker"&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;draggable&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
  &lt;span class="nx"&gt;start&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;event&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;ui&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;//Handler when marker start moving&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nx"&gt;drag&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;event&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;ui&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;//Handler when marker is on move&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nx"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;event&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;ui&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;//Hander when marker is stopped moving&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Normally we can use &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;drag&lt;/code&gt; functions to handle animation when marker is on moved, and when marker is stopped moving, &lt;code&gt;stop&lt;/code&gt; function is triggered, and we will update markers info to database and update markers of the video with new positions.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;player&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;markers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;removeAll&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="nx"&gt;$&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'#video_video .vjs-marker'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;remove&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="nx"&gt;player&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;markers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;markers&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/recording-player-markers/marker_drag.gif" alt="nil"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org51fdcf3" class="outline-2"&gt;
&lt;h2 id="org51fdcf3"&gt;Marker's Most Common Events&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org51fdcf3"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbb8288f" class="outline-3"&gt;
&lt;h3 id="orgbb8288f"&gt;onMarkerReached&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbb8288f"&gt;
&lt;p&gt;
The event will be fired when the current position of the videojs player reaches a marker position. Please be noticed that there can be a time difference and the event is not raised precisely at marker's position time. The different can be up to 0.01 second.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7488341" class="outline-3"&gt;
&lt;h3 id="org7488341"&gt;onMarkerClick&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7488341"&gt;
&lt;p&gt;
The event is fired when a videojs marker is clicked and you can get the data of the clicked marker through the function argument.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>JavaScript</category><guid>https://cocktailmake.github.io/posts/recording-player-markers/</guid><pubDate>Fri, 19 Mar 2021 15:00:00 GMT</pubDate></item><item><title>FFmpeg WASM</title><link>https://cocktailmake.github.io/posts/ffmpeg-wasm/</link><dc:creator>Duc To</dc:creator><description>&lt;div id="outline-container-org3632b1d" class="outline-2"&gt;
&lt;h2 id="org3632b1d"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3632b1d"&gt;
&lt;p&gt;
&lt;b&gt;FFmpeg WASM&lt;/b&gt; is a pure WebAssembly / JavaScript port of FFmpeg. It enables video &amp;amp; audio record, convert and stream right inside browsers.
The main Git repository for it is &lt;a href="https://github.com/ffmpegwasm/ffmpeg.wasm"&gt;https://github.com/ffmpegwasm/ffmpeg.wasm&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/ffmpeg-wasm/ffmpeg_wasm.png" alt="nil"&gt;
&lt;/p&gt;

&lt;p&gt;
Right now, in Recording process, we utilize user experience by uploading video source file directly from front-end web browser to S3 without going through any server using presigned S3 URL. The limitation of the approach is that we cannot convert the source video file as needed without going through the server (which increases a lot of waiting time for end users). Some handy cases for that needs are:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;Unify video source format to MP4 from different video file formats (AVI, MOV).&lt;/li&gt;
&lt;li&gt;Right now web browser does not support H265 video format and we need to convert to MP4 format to support playing it in web browser.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Based on the above needs, we research &lt;b&gt;FFmpeg WASM&lt;/b&gt; library as we think that it would be nice if we can do the video conversion directly from web browser
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgecd3836" class="outline-2"&gt;
&lt;h2 id="orgecd3836"&gt;Licensing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgecd3836"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;@ffmpeg/ffmpeg&lt;/code&gt; contains kind of a wrapper to handle the complexity of loading core and calling low-level APIs. It is a small code base and under MIT license.&lt;/li&gt;

&lt;li&gt;&lt;code&gt;@ffmpeg/core&lt;/code&gt; following the same licenses as FFmpeg and its external libraries.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org88d916c" class="outline-2"&gt;
&lt;h2 id="org88d916c"&gt;How it works&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org88d916c"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;The only JavaScript file defined in HTML file is &lt;a href="https://unpkg.com/@ffmpeg/ffmpeg@0.9.4/dist/ffmpeg.min.js"&gt;https://unpkg.com/@ffmpeg/ffmpeg@0.9.4/dist/ffmpeg.min.js&lt;/a&gt; but actually it is just a wrapper.&lt;/li&gt;
&lt;li&gt;Based on the actual need in our triggered ffmpeg function, the JavaScript will load ffmpeg/core file &lt;code&gt;*ffmpeg-core.wasm&lt;/code&gt; and its JavaScript file &lt;code&gt;*ffmpeg-core.js&lt;/code&gt;. The size of &lt;code&gt;ffmpeg-core.wasm&lt;/code&gt; at the time of testing is 22MB.&lt;/li&gt;
&lt;li&gt;Sometimes we may need to use the newest version of &lt;code&gt;@ffmpeg/core&lt;/code&gt; and we can define it using customized path.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kr"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;ffmpeg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;createFFmpeg&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
  &lt;span class="nx"&gt;corePath&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'../../../src/ffmpeg-core.js'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;We can re-compile ffmpeg-core by checking &lt;b&gt;build.sh&lt;/b&gt; inside &lt;a href="https://github.com/ffmpegwasm/ffmpeg.wasm-core"&gt;https://github.com/ffmpegwasm/ffmpeg.wasm-core&lt;/a&gt; repository.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge4ea929" class="outline-2"&gt;
&lt;h2 id="orge4ea929"&gt;Testing example&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge4ea929"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;In the demo below, user will have an upload button to upload source video file, which can be in either webm, MOV or H.265 MP4 format.&lt;/li&gt;
&lt;li&gt;It takes several minutes to transcode and you can monitor the progress on the console of DevTool.&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe src="https://test.prototype.richka.co/~duc/wasm/" style="width:100%; height: 400px;"&gt;&lt;/iframe&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;The source of the sample demo is below.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;body&amp;gt;
  &amp;lt;video id="player" controls&amp;gt;&amp;lt;/video&amp;gt;
  &amp;lt;input type="file" id="uploader"&amp;gt;
  &amp;lt;script src="https://unpkg.com/@ffmpeg/ffmpeg@0.9.4/dist/ffmpeg.min.js"&amp;gt;&amp;lt;/script&amp;gt;
  &amp;lt;script&amp;gt;
    const { createFFmpeg, fetchFile } = FFmpeg;
    const ffmpeg = createFFmpeg({ log: true });
    const transcode = async ({ target: { files } }) =&amp;gt; {
      const { name } = files[0];
      await ffmpeg.load();
      ffmpeg.FS('writeFile', name, await fetchFile(files[0]));
      await ffmpeg.run('-i', name,  'output.mp4');
      //await ffmpeg.run('-i', name, '-q:v', 0, 'output.mp4');
      const data = ffmpeg.FS('readFile', 'output.mp4');
      const video = document.getElementById('player');
      video.src = URL.createObjectURL(new Blob([data.buffer], { type: 'video/mp4' }));
    }

    document.getElementById('uploader').addEventListener('change', transcode);
  &amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;When user click upload button and select a MOV video file, &lt;b&gt;FFmpeg WASM&lt;/b&gt; library will be called to converted it to MP4 file (output.mp4).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/ffmpeg-wasm/wasm_upload.png" alt="nil"&gt;
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;User can trace the conversion status, as normal ffmpeg oss by its output in console log&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/ffmpeg-wasm/console_log1.png" alt="nil"&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/ffmpeg-wasm/console_log2.png" alt="nil"&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/ffmpeg-wasm/console_log3.png" alt="nil"&gt;
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;When the conversion is completed, JS loads HTML video tag's source with the converted video's data and display on screen.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/ffmpeg-wasm/conversion_output.png" alt="nil"&gt;
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;User flow diagram can be described as below&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/ffmpeg-wasm/wasm_figure1.png" alt="nil"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5726503" class="outline-2"&gt;
&lt;h2 id="org5726503"&gt;Testing result&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5726503"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;We made 4 test cases converting from different source video format to MP4 format:
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;b&gt;WebM&lt;/b&gt;: It is successful and the conversion time is fast. For around 8MB video it took less than 2 minutes.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;H.265&lt;/b&gt;: It is successful but the progress takes very long to complete, more than 6 minutes while if we do it with ffmpeg command it is just around 3 minutes. So nearly double the time.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;General MOV file from Internet&lt;/b&gt;: It is successful and the conversion time is quite fast, around 2 minutes.&lt;/li&gt;
&lt;li&gt;MOV file which recorded from MacOS: It is failed after few seconds using &lt;b&gt;FFmpeg WASM&lt;/b&gt; library but it is successful using ffmpeg command.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;General MOV file from Internet&lt;/b&gt;: It is successful and the conversion time is quite fast, around 2 minutes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
At the moment of writing this article, &lt;b&gt;FFmpeg WASM&lt;/b&gt; is twice slower and unstable, but it is worth monitoring the future progress, and we may use WASM of ImageMagic in the future.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>JavaScript</category><category>wasm</category><guid>https://cocktailmake.github.io/posts/ffmpeg-wasm/</guid><pubDate>Wed, 06 Jan 2021 05:00:00 GMT</pubDate></item><item><title>Our Git Habit of Narration Recording project</title><link>https://cocktailmake.github.io/posts/narration-recording-git-habit/</link><dc:creator>Duc To</dc:creator><description>&lt;div id="outline-container-orgefcd2c7" class="outline-2"&gt;
&lt;h2 id="orgefcd2c7"&gt;Abstract&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgefcd2c7"&gt;
&lt;p&gt;
We released a new web application called &lt;b&gt;&lt;span style="color: red"&gt;ãã¬æ®ã&lt;/span&gt;&lt;/b&gt;
which enables users to record the voice as narration on web browsers
and combine them with video sources and generate videos in last month.
In this post, we introduce our daily Git habit how to handle multiple
branches to add new features and release official versions and hotfix
releasing for bug fixing as professional development.
&lt;/p&gt;

&lt;p&gt;
Our development team refers to a good article of &lt;a href="https://nvie.com/posts/a-successful-git-branching-model/"&gt;Git Workflow&lt;/a&gt; and the
format of commit log of &lt;a href="https://github.com/angular/angular/blob/master/CONTRIBUTING.md#commit"&gt;AngularJS guys&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/narration-recording-git-habit/git-model@2x.png" alt="nil"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge7f2ed0" class="outline-2"&gt;
&lt;h2 id="orge7f2ed0"&gt;Branches&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge7f2ed0"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1003e5e" class="outline-3"&gt;
&lt;h3 id="org1003e5e"&gt;Feature branches&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1003e5e"&gt;
&lt;p&gt;
When you develop new features or fix bugs and the total modified lines
will be relatively larger compared with prior ones you developed, then
you should create new branches and merge after finishing the
developments as general Git culture.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4b1fd48" class="outline-3"&gt;
&lt;h3 id="org4b1fd48"&gt;Stage branch&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4b1fd48"&gt;
&lt;p&gt;
It is a primary development branch and feature branches will be merged
to stage branch after successfully tested and reviewed.
&lt;/p&gt;

&lt;p&gt;
This branch is a head of any other branches, but there is a
possibility any degrade happens with merging the working branches into
this branch. Therefore we can't release this branch as official
version right now.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org160236d" class="outline-3"&gt;
&lt;h3 id="org160236d"&gt;Release branches&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org160236d"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;On each release, we create a new release branch based on latest
stage branch for both recording and video repositories (even though
there may not have any changes since last release for one
repository).&lt;/li&gt;
&lt;li&gt;The release branch should be named as &lt;b&gt;&lt;b&gt;release/[VERSION_NUMBER]&lt;/b&gt;&lt;/b&gt;.&lt;/li&gt;
&lt;li&gt;Sometimes a hotfix releasing is needed and we can apply hotfix commits to
the release branches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/narration-recording-git-habit/hotfix-branches@2x.png" alt="nil"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0d02ab1" class="outline-3"&gt;
&lt;h3 id="org0d02ab1"&gt;Tags&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0d02ab1"&gt;
&lt;p&gt;
Tags are created with the release branches of both recording and video repositories
after the final reviewing the release has been successfully passed.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/narration-recording-git-habit/git-model@2x.png" alt="nil"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbe946ce" class="outline-2"&gt;
&lt;h2 id="orgbe946ce"&gt;Commit logs&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgbe946ce"&gt;
&lt;p&gt;
We apply &lt;a href="https://github.com/angular/angular/blob/master/CONTRIBUTING.md#commit"&gt;3rd party knowledge&lt;/a&gt; for the format of commit logs from AngularJS guys.
We follow the tiny rule in the article as below.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;type&amp;gt;&lt;span class="o"&gt;(&lt;/span&gt;&amp;lt;scope&amp;gt;&lt;span class="o"&gt;)&lt;/span&gt;: &amp;lt;subject&amp;gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Type : Must be one of the following:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;| Type     | Meaning                                                                                                |
|----------+--------------------------------------------------------------------------------------------------------|
| feat     | A new feature                                                                                          |
| fix      | A bug fix                                                                                              |
| docs     | Documentation only changes                                                                             |
| style    | Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) |
| refactor | A code change that neither fixes a bug nor adds a feature                                              |
| perf     | A code change that improves performance                                                                |
| test     | Adding missing or correcting existing tests                                                            |
| chore    | Changes to the build process or auxiliary tools and libraries such as documentation generation         |
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
For examples, our commit logs would be below.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fix(gui): Fixed file uploading. Refs #REC-XXX
fix(video): Make ffmpeg video generation more stable. Refs #REC-XXX
test(django): Added new tests for admin contract page. Refs #REC-XXX
feat(gui) : Added Narration script function. Refs #REC-XXX 
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc4aa91d" class="outline-2"&gt;
&lt;h2 id="orgc4aa91d"&gt;Utilize special keywords in commit logs&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc4aa91d"&gt;
&lt;p&gt;
When we commit, we fill understandable commit logs and use special
keywords such as Refs #TICKET_NUMBER to make the references with the
corresponding tickets.  Therefore before committing, we create tickets
at first and make the references with the commit logs.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcca7d04" class="outline-2"&gt;
&lt;h2 id="orgcca7d04"&gt;Rules for merging&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgcca7d04"&gt;
&lt;p&gt;
When we merge branches, we have a rule to add "&lt;b&gt;&lt;b&gt;âno-ff&lt;/b&gt;&lt;/b&gt;" as command
option to suppress fast-forward of the default behavior of Git.
&lt;/p&gt;

&lt;p&gt;
For examples: &lt;code&gt;git merge --no-ff feature/REC-00&lt;/code&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf39a98b" class="outline-2"&gt;
&lt;h2 id="orgf39a98b"&gt;Modification of DB models&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf39a98b"&gt;
&lt;p&gt;
When we modify the models of Django with development tasks, we do that
on stage branch at first. We sometimes encountered DB migration issue
on stage branch and it also caused runtime issues on our individual
working branches. To prevent from happening the same issue, we applied
a simple work flow to modify DB models only on stage branch.
&lt;/p&gt;

&lt;ol class="org-ol"&gt;
&lt;li&gt;If you need to modify models of Django, checkout stage branch at first&lt;/li&gt;
&lt;li&gt;Modify the models and execute migration against staging DB&lt;/li&gt;
&lt;li&gt;Merge stage branch into your working branches&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>git</category><guid>https://cocktailmake.github.io/posts/narration-recording-git-habit/</guid><pubDate>Sun, 20 Sep 2020 03:00:00 GMT</pubDate></item><item><title>Released Narration Recording Service</title><link>https://cocktailmake.github.io/posts/released-narration-recording-service/</link><dc:creator>Hitoshi Uchida</dc:creator><description>&lt;div id="outline-container-orgffb2583" class="outline-2"&gt;
&lt;h2 id="orgffb2583"&gt;Abstract&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgffb2583"&gt;
&lt;p&gt;
Some RICHKA users requested that they would like to input their
narration into the videos and recently we released a new web
application called &lt;b&gt;&lt;span style="color: red"&gt;ãã¬æ®ã&lt;/span&gt;&lt;/b&gt; which enables users to
record the voice as narration on web browsers and combine them with
video sources and generate videos. And &lt;b&gt;synthesis of speech
technology&lt;/b&gt; to automatically generate voice data with the input texts
is also supported as &lt;b&gt;AI narration mode&lt;/b&gt; and users
don't have input voice.
&lt;/p&gt;

&lt;p&gt;
The screenshot below is the sample of the edit page. The upper left is
a dedicated video player to playback both a video source and recorded
narration with combining them. Users can edit the start time of cues
with dragging the cue point on the seek bar. When users click the
recording button in the lower left, the input voice is recorded
through WebRTC and it is converted to MP3 on the web browsers. The
right side is the narration texts users can add/edit. In the AI
narration mode, the voices are generated with the texts.
&lt;/p&gt;

&lt;p&gt;
After the recording has been done, users click the generation button
in the upper right and it starts to generate a video with combining
the video source and the recorded voices.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/released-narration-recording-service/sample-edit-page.png" alt="nil"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8a60249" class="outline-2"&gt;
&lt;h2 id="org8a60249"&gt;Development Environment&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8a60249"&gt;
&lt;p&gt;
Web servers are based on Django, and additional 8 Python packages such
as Django extensions and boto3 to publish pre-signed S3 URL and WebVTT
parser are integrated. The current total lines of Django is around
3000 and it is still quite small because it is still beta version.
Regarding the front end, 20 OSS libraries such as jQuery, videojs,
videojs marker, RecordRTC etc. are integrated. The current total lines
of JavaScript codes is around 4300 and the font end is also still
quite small.
&lt;/p&gt;

&lt;p&gt;
This product is still beta, and based on the feedback from users, new
advanced features will be continuously added and this project will
also become big service soon such as RICHKA. Regarding the synthesis
of speech technology, we will make a new post and share the detail.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/released-narration-recording-service/technologies.png" alt="nil"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org40154b4" class="outline-2"&gt;
&lt;h2 id="org40154b4"&gt;Voice Recording&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org40154b4"&gt;
&lt;p&gt;
One of primary features is to record voice per cue point inputted
with the mic. When users click the recording button, the recording
processing is executed with using WebRTC as a diagram below.
&lt;/p&gt;

&lt;ol class="org-ol"&gt;
&lt;li&gt;A user clicks the recording button and records the voice with mic&lt;/li&gt;
&lt;li&gt;The voice data is retrieved with WebRTC API and converted into
MP3 in JavaScript layer. Then it is directly uploaded to S3 with
pre-signed S3 URL.&lt;/li&gt;
&lt;li&gt;To enable the user to listen the recorded voice, our dedicated video
player on JavaScript layer loads the record and initialize to be ready.&lt;/li&gt;
&lt;li&gt;The user can playback the video source with overlaying the recorded
voices without generating a new video on the dedicated video
player.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/released-narration-recording-service/voice-recording.png" alt="nil"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd42815e" class="outline-3"&gt;
&lt;h3 id="orgd42815e"&gt;Direct Conversion to MP3 on JavaScript layer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd42815e"&gt;
&lt;p&gt;
At the 1st step, the MIME type of the audio data retrieved by WebRTC
is audio/webm in default and it tended to be large data size and the
quality of the voice is a bit higher quality. To adjust the quality
and the data size to match our requirements, we decided to use a
JavaScript library RecordRTC to directly convert to MP3 on JavaScript
layer and upload to S3 without delegating the conversion processing to
servers and Lambda. After we obtain the binary of MP3, we don't
convert to other formats in the data life cycle. The client processing
makes the architecture simpler and doesn't cause any additional load
to the server side.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5f26fb8" class="outline-3"&gt;
&lt;h3 id="org5f26fb8"&gt;Dedicated video player to sync video and voices&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5f26fb8"&gt;
&lt;p&gt;
At the 3rd step, we implemented a dedicated video player to playback
the video source with overlaying the recorded voices without
generating another video. The advantage is users can immediately check
the recording results without waiting for several seconds to generate
new videos. The dedicated video player has internally two players to
playback with synchronizing the video source and the recorded voices.
&lt;/p&gt;

&lt;p&gt;
When the current seek point reaches the next cue point, the video
player loads the corresponding voice data from S3 and make the video
player ready to play.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org24bb778" class="outline-2"&gt;
&lt;h2 id="org24bb778"&gt;Video Generation&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org24bb778"&gt;
&lt;p&gt;
After users have inputted the voices to cue points, user are ready to
generate videos with overlying recorded voices. The generated ones
can be downloaded as independent video file.
&lt;/p&gt;

&lt;p&gt;
When users click the generation button, the generation process is
executed on one of dedicated video servers as steps below.
&lt;/p&gt;

&lt;ol class="org-ol"&gt;
&lt;li&gt;When a user click the generation button, an HTTP POST request is
sent to one of web servers behind of a load balancer.&lt;/li&gt;
&lt;li&gt;The web server retrieves the location of the corresponding voices
of S3 and sends an HTTP request to one of video servers.&lt;/li&gt;
&lt;li&gt;The video server downloads the video source and the recorded voice
files from S3, and generate an MP4 video with overlying the voices
over the video source with using ffmpeg.&lt;/li&gt;
&lt;li&gt;The video server uploads the generated video to S3 with pre-singed S3 URL.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/released-narration-recording-service/video-generation.png" alt="nil"&gt;
&lt;/p&gt;


&lt;p&gt;
At 3rd step to generate the video, the simplified sample command of
ffmpeg is like below.
&lt;/p&gt;

&lt;p target="_blank"&gt;
Each recorded voice stream is overlaid over the audio stream of
the video source with &lt;a href="https://ffmpeg.org/ffmpeg-filters.html#amerge-1" target="_blank"&gt;&lt;b&gt;amerge&lt;/b&gt;&lt;/a&gt; command to multiplex.
&lt;/p&gt;

&lt;p target="_blank"&gt;
And they are concatenated into one audio stream with &lt;a href="https://ffmpeg.org/ffmpeg-filters.html#concat" target="_blank"&gt;&lt;b&gt;concat&lt;/b&gt;&lt;/a&gt; command
as "[m1][s1][m2][s2][m3][m4][s3]concat=n=7:v=0:a=1[out]" in the
filter_complex option.
&lt;/p&gt;

&lt;p target="_blank"&gt;
To keep the original video stream of the video source, the video
stream is directly copied to the output stream with enabling stream
copy mode with &lt;a href="https://ffmpeg.org/ffmpeg.html#Stream-copy" target="_blank"&gt;&lt;b&gt;-c:vcopy&lt;/b&gt;&lt;/a&gt; option. It can avoid needless encoding of
video stream and suppress CPU usage, therefore this command can be
rapidly done.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ffmpeg -i 'vide_source.mp4'
-i 'voice_1.mp3'
-i 'voice_2.mp3'
-i 'voice_3.mp3'
-i 'voice_4.mp3'
-filter_complex '[0:a]atrim=start=0.0:duration=1.87,aformat=sample_fmts=fltp:sample_rates=44100:
channel_layouts=stereo,volume=1,asetpts=PTS-STARTPTS[sm1];[0:a]atrim=start=1.87:duration=0.63,
aformat=sample_fmts=fltp:sample_rates=44100:channel_layouts=stereo,volume=1,asetpts=PTS-STARTPTS[s1];
[0:a]atrim=start=2.5:duration=1.44,aformat=sample_fmts=fltp:sample_rates=44100:channel_layouts=stereo,
volume=1,asetpts=PTS-STARTPTS[sm2];[0:a]atrim=start=3.94:duration=1.56,aformat=sample_fmts=fltp:
sample_rates=44100:channel_layouts=stereo,volume=1,asetpts=PTS-STARTPTS[s2];[0:a]atrim=start=5.5:
duration=2.0,aformat=sample_fmts=fltp:sample_rates=44100:channel_layouts=stereo,volume=1,
asetpts=PTS-STARTPTS[sm3];[0:a]atrim=start=7.5:duration=1.82,aformat=sample_fmts=fltp:sample_rates=44100:
channel_layouts=stereo,volume=1,asetpts=PTS-STARTPTS[sm4];[0:a]atrim=start=9.32,aformat=sample_fmts=fltp:
sample_rates=44100:channel_layouts=stereo,volume=1,asetpts=PTS-STARTPTS[s3];[sm1][1:a]amerge[m1];
[sm2][2:a]amerge[m2];[sm3][3:a]amerge[m3];[sm4][4:a]amerge[m4];[m1][s1][m2][s2][m3][m4][s3]concat=n=7:v=0:a=1[out]'
-c:v copy -map 0:v -map [out] 'out.mp4'
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The figure below represents how merging and concatenating audio
streams work with the ffmpeg command. The concatenated audio stream is
accumulated into an output stream [out] in the command. Then, it is
combined with the video stream of the video source with &lt;b&gt;-c:v copy
-map 0:v -map [out]&lt;/b&gt; and the final result is serialized into a file
out.mp4.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="https://cocktailmake.github.io/images/released-narration-recording-service/audio-merge.png" alt="nil"&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><guid>https://cocktailmake.github.io/posts/released-narration-recording-service/</guid><pubDate>Wed, 12 Aug 2020 06:38:24 GMT</pubDate></item></channel></rss>